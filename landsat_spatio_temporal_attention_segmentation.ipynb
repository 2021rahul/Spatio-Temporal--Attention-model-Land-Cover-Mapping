{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "landsat_spatio_temporal_attention_segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzaPLyR7I8RU"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import gdal\n",
        "from osgeo import gdal, gdalconst, osr\n",
        "import time\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMLYfexjuc1X"
      },
      "source": [
        "experiment_id = 'landsat_2017_spatio_temporal_attention_segmentation'\n",
        "no_timestamps = time_steps = 10\n",
        "patch_size = (32,32)\n",
        "input_patch_size = 32\n",
        "label_patch_size = (16,16)\n",
        "labels_list = [0,1,2,3,4,5,6,7,8,9,10,11] #list of labels to use while calculating f1 score\n",
        "step_size = 16\n",
        "output_patch_width = 16\n",
        "no_features = channels =  7\n",
        "no_of_classes = 12\n",
        "learning_rate = 0.0001\n",
        "max_accuracy_test = 0 \n",
        "diff = 8 \n",
        "no_of_epochs = 100\n",
        "model_folder = 'models_landsat_2017_spatio_temporal_attention_segmentation/' + experiment_id\n",
        "if not os.path.exists(model_folder):\n",
        "  os.makedirs(model_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x2QrrBI5Y_X",
        "outputId": "aa1f1355-bd42-466c-cbc5-c4f868bba606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "temporal_images_array = np.load(os.path.join(\"data/landsat_processed_data/numpy_arrays\",\"landsat_2017_multicrop_series.npy\"))\n",
        "print(temporal_images_array.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 2976, 3712, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICO3sZGVvZb0",
        "outputId": "de363367-fed7-4619-d979-9620795de9bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "## Read and Prepare labels\n",
        "raster = gdal.Open(os.path.join(\"data/labels\",  \"usda_labels_2017_multicrop.tif\"))\n",
        "raw_label = raster.ReadAsArray()\n",
        "print('Unique labels are : ', np.unique(raw_label))\n",
        "# print('Count of each label: ', np.bincount(raw_label.flatten().astype(int)))\n",
        "\n",
        "#convert raw labels to classes\n",
        "label = np.zeros((2976,3712))\n",
        "label[(raw_label==1) | (raw_label==12) | (raw_label==13)] = 1                               # corn               \n",
        "label[raw_label==5] = 2                                                                     # soybean\n",
        "label[raw_label==41] = 3                                                                    # sugarbeets\n",
        "label[(raw_label==23) | (raw_label==24) | (raw_label==39)] = 4                              # wheat\n",
        "label[(raw_label==42)] = 5                                                                  # drybean\n",
        "label[(raw_label==36)] = 6                                                                  # Alfa Alfa\n",
        "label[(raw_label==53)] = 7                                                                  # Peas\n",
        "label[(raw_label==111)] = 8                                                                 # Open water\n",
        "label[(raw_label==121) | (raw_label==122) | (raw_label==123) | (raw_label==124)] = 9        # Developed Areas\n",
        "label[(raw_label==141) | (raw_label==142) | (raw_label==143)] = 10                          # forests\n",
        "label[(raw_label==190) | (raw_label==195) | (raw_label==176) | (raw_label==152) | (raw_label==59)| (raw_label==60) | (raw_label==58) ] = 11 #Wetlands and grass\n",
        "\n",
        "print('label shape: ',label.shape)\n",
        "print('New unique labels are : ', np.unique(label))\n",
        "print('Count of each label: ', np.bincount(label.flatten().astype(int)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique labels are :  [  1   4   5   6  21  22  23  24  27  28  29  30  31  36  37  39  41  42\n",
            "  43  44  53  58  59  60  61  68 111 121 122 123 124 131 141 142 143 152\n",
            " 176 190 195 205 229]\n",
            "label shape:  (2976, 3712)\n",
            "New unique labels are :  [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11.]\n",
            "Count of each label:  [  56600 4314334 3461701  466977   54804   88877  201008   25160  308932\n",
            "  621749  391754 1055016]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Sr0r65cKvZ5",
        "outputId": "8d81f0ba-f599-4986-baad-dd86355e3843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "testing_split = 0.5\n",
        "height = temporal_images_array.shape[1]\n",
        "train_image_segment = temporal_images_array[:,:int(height * (1-testing_split)), :, :7] # extract the bands of interest\n",
        "train_label_segment = label[:int(height * (1-testing_split)), :]  \n",
        "test_image_segment = temporal_images_array[:,int(height * (1-testing_split)):, :, :7] # extract the bands of interest\n",
        "test_label_segment = label[int(height * (1-testing_split)):, :]\n",
        "print('Printing shapes')\n",
        "print('Train images shape: ',train_image_segment.shape,'Train labels shape: ',train_label_segment.shape)\n",
        "print('Test images shape: ',test_image_segment.shape,'Test labels shape: ',test_label_segment.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing shapes\n",
            "Train images shape:  (10, 1488, 3712, 7) Train labels shape:  (1488, 3712)\n",
            "Test images shape:  (10, 1488, 3712, 7) Test labels shape:  (1488, 3712)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrJ9GwKcx_hp"
      },
      "source": [
        "# Model Architecture\n",
        "\n",
        "class UNET_LSTM_BIDIRECTIONAL_ATTENTION(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNET_LSTM_BIDIRECTIONAL_ATTENTION,self).__init__()\n",
        "\n",
        "        self.conv1_1 = torch.nn.Conv2d(in_channels, 64, 3, padding=1)\n",
        "        self.conv1_2 = torch.nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.conv2_1 = torch.nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv2_2 = torch.nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.conv3_1 = torch.nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv3_2 = torch.nn.Conv2d(256, 256, 3, padding=1)\n",
        "\n",
        "        self.lstm = torch.nn.LSTM(256, 256, batch_first=True, bidirectional=True)\n",
        "        self.attention = torch.nn.Linear(512, 1)\n",
        "\n",
        "        self.unpool2 = torch.nn.ConvTranspose2d(512 , 128, kernel_size=2, stride=2)\n",
        "        self.upconv2_1 = torch.nn.Conv2d(256, 128, 3, padding=1)\n",
        "        self.upconv2_2 = torch.nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.unpool1 = torch.nn.ConvTranspose2d(128 , 64, kernel_size=2, stride=2)\n",
        "        self.upconv1_1 = torch.nn.Conv2d(128, 64, 3, padding=1)\n",
        "        self.upconv1_2 = torch.nn.Conv2d(64, 64, 3, padding=1)\n",
        "\n",
        "        self.out = torch.nn.Conv2d(64, out_channels, kernel_size=1, padding=0)\n",
        "\n",
        "        self.maxpool = torch.nn.MaxPool2d(2)\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "        self.dropout = torch.nn.Dropout(p=0.1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)        \n",
        "    \n",
        "    def crop_and_concat(self, x1, x2):\n",
        "        x1_shape = x1.shape\n",
        "        x2_shape = x2.shape\n",
        "        offset_2, offset_3 = (x1_shape[2]-x2_shape[2])//2, (x1_shape[3]-x2_shape[3])//2\n",
        "        x1_crop = x1[:, :, offset_2:offset_2+x2_shape[2], offset_3:offset_3+x2_shape[3]]\n",
        "        return torch.cat([x1_crop, x2], dim=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "      \n",
        "        x = x.view(-1, channels, input_patch_size, input_patch_size)\n",
        "\n",
        "        conv1 = self.relu(self.conv1_2(self.relu(self.conv1_1(x))))\n",
        "        maxpool1 = self.maxpool(conv1)\n",
        "        conv2 = self.relu(self.conv2_2(self.relu(self.conv2_1(maxpool1))))\n",
        "        maxpool2 = self.maxpool(conv2)\n",
        "        conv3 = self.relu(self.conv3_2(self.relu(self.conv3_1(maxpool2))))\n",
        "\n",
        "        shape_enc = conv3.shape\n",
        "        conv3 = conv3.view(-1, time_steps, conv3.shape[1], conv3.shape[2]*conv3.shape[3]) \n",
        "        conv3 = conv3.permute(0,3,1,2) \n",
        "        conv3 = conv3.reshape(conv3.shape[0]*conv3.shape[1], time_steps, 256)\n",
        "        lstm, _ = self.lstm(conv3) \n",
        "        lstm = self.relu(lstm.reshape(-1, 512))\n",
        "        attention_weights = torch.nn.functional.softmax(torch.squeeze(torch.nn.functional.avg_pool2d(self.attention(torch.tanh(lstm)).view(-1,shape_enc[2],shape_enc[3],time_steps).permute(0,3,1,2), 8)), dim=1) \n",
        "        context = torch.sum((attention_weights.view(-1, 1, 1, time_steps).repeat(1, 8, 8, 1).view(-1, 1)*lstm).view(-1, time_steps, 512), dim=1).view(-1,shape_enc[2],shape_enc[3], 512).permute(0,3,1,2)\n",
        "\n",
        "        attention_weights_fixed = attention_weights.detach()\n",
        "        unpool2 = self.unpool2(context)\n",
        "        agg_conv2 = torch.sum(attention_weights_fixed.view(-1, time_steps, 1, 1, 1) * conv2.view(-1, time_steps, conv2.shape[1], conv2.shape[2], conv2.shape[3]), dim=1)\n",
        "        upconv2 = self.relu(self.upconv2_2(self.relu(self.upconv2_1(self.crop_and_concat(agg_conv2, unpool2)))))\n",
        "        unpool1 = self.unpool1(upconv2)\n",
        "        agg_conv1 = torch.sum(attention_weights_fixed.view(-1, time_steps, 1, 1, 1) * conv1.view(-1, time_steps, conv1.shape[1], conv1.shape[2], conv1.shape[3]), dim=1)\n",
        "        upconv1 = self.relu(self.upconv1_2(self.relu(self.upconv1_1(self.crop_and_concat(agg_conv1, unpool1)))))\n",
        "        out = self.out(upconv1)\n",
        "\n",
        "        return out[:,:,diff:-diff, diff:-diff]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxT4_q_Hn5xb"
      },
      "source": [
        "# build model\n",
        "model = UNET_LSTM_BIDIRECTIONAL_ATTENTION(in_channels=no_features, out_channels=no_of_classes)\n",
        "model = model.to('cuda')\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "## train model\n",
        "train_loss = []\n",
        "train_accuracy = []\n",
        "test_loss = []\n",
        "test_accuracy = []\n",
        "no_of_patches_x = int((train_image_segment.shape[1] - (patch_size[0]))/step_size)\n",
        "no_of_patches_y = int((train_image_segment.shape[2] - (patch_size[1]))/step_size)\n",
        "no_of_patches_x_test = int((test_image_segment.shape[1] - (patch_size[0]))/step_size)\n",
        "no_of_patches_y_test = int((test_image_segment.shape[2] - (patch_size[1]))/step_size)\n",
        "w = int((patch_size[0]-label_patch_size[0])/2)\n",
        "image_batch = np.zeros((no_of_patches_y, ) + (no_timestamps, ) + patch_size + (no_features, ))\n",
        "label_batch = np.zeros((no_of_patches_y, ) + label_patch_size)\n",
        "\n",
        "for epoch in range(no_of_epochs):\n",
        "  # Train  \n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  accuracy = 0\n",
        "  start_time = time.time()\n",
        "\n",
        "  for x in range(no_of_patches_x):  \n",
        "\n",
        "    for y in range(no_of_patches_y):\n",
        "      \n",
        "      for t in range(no_timestamps):\n",
        "        image_batch[y][t] = train_image_segment[ t, x*step_size:(x*step_size) + patch_size[0], y*step_size:(y*step_size) + patch_size[1], :] \n",
        "      label_batch[y] = train_label_segment[x*step_size + w:(x*step_size) + w + label_patch_size[0], y*step_size + w:(y*step_size) + w + label_patch_size[1]]\n",
        "\n",
        "    image_batch_tr = np.transpose(image_batch,(0,1,4,2,3)) \n",
        "    image_batch_t = torch.Tensor(image_batch_tr)\n",
        "    label_batch_t = torch.Tensor(label_batch)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    patch_out = model(image_batch_t.to('cuda'))\n",
        "    label_batch_t = label_batch_t.type(torch.long).to('cuda')\n",
        "    loss = criterion(patch_out, label_batch_t)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    patch_out_pred =  torch.argmax(torch.nn.functional.softmax(patch_out, dim=1), dim=1)\n",
        "    total_loss += loss.item()\n",
        "    patch_out_pred = np.reshape(patch_out_pred.cpu().numpy(), (-1))\n",
        "    label_batch_t = np.reshape(label_batch_t.cpu().numpy(), (-1))\n",
        "    \n",
        "    accuracy += accuracy_score(patch_out_pred,label_batch_t)\n",
        "  \n",
        "  print('\\nEpoch {0}:\\t Loss: {1}\\t Accuracy: {2}\\t Time: {3}'.format(epoch, total_loss/(no_of_patches_x), (accuracy/no_of_patches_x), time.time() - start_time))\n",
        "  train_loss.append(total_loss/(no_of_patches_x))\n",
        "  train_accuracy.append(accuracy/(no_of_patches_x))\n",
        "\n",
        "  # Test\n",
        "  model.eval()\n",
        "  total_loss_test = 0\n",
        "  accuracy_test = 0\n",
        "  start_time = time.time()\n",
        "  pred_list = []\n",
        "  true_list = []\n",
        "\n",
        "  for x in range(no_of_patches_x_test):  \n",
        "\n",
        "    for y in range(no_of_patches_y):\n",
        "\n",
        "      for t in range(no_timestamps):\n",
        "        image_batch[y][t] = test_image_segment[ t, x*step_size:(x*step_size) + patch_size[0], y*step_size:(y*step_size) + patch_size[1], :] \n",
        "      label_batch[y] = test_label_segment[x*step_size + w:(x*step_size) + w + label_patch_size[0], y*step_size + w:(y*step_size) + w + label_patch_size[1]]\n",
        "    \n",
        "    image_batch_tr = np.transpose(image_batch,(0,1,4,2,3)) \n",
        "    image_batch_t = torch.Tensor(image_batch_tr)\n",
        "    label_batch_t = torch.Tensor(label_batch)\n",
        "\n",
        "    # optimizer.zero_grad()\n",
        "    patch_out = model(image_batch_t.to('cuda'))\n",
        "    label_batch_t = label_batch_t.type(torch.long).to('cuda')\n",
        "    patch_out_pred =  torch.argmax(torch.nn.functional.softmax(patch_out, dim=1), dim=1)\n",
        "    total_loss_test += loss.item()\n",
        "    patch_out_pred = np.reshape(patch_out_pred.cpu().numpy(), (-1))\n",
        "    label_batch_t = np.reshape(label_batch_t.cpu().numpy(), (-1))\n",
        "    \n",
        "    accuracy_test += accuracy_score(patch_out_pred,label_batch_t)\n",
        "    pred_list.append(patch_out_pred)\n",
        "    true_list.append(label_batch_t)\n",
        "\n",
        "  pred_list_arr = np.array(pred_list).reshape(-1)\n",
        "  true_list_arr = np.array(true_list).reshape(-1)\n",
        "  mean_f1_score = np.mean(f1_score(true_list_arr,pred_list_arr,average = None,labels=labels_list))\n",
        "  print('Test   :\\t Loss: {1}\\t Accuracy: {2}\\t Time: {3}'.format(epoch, total_loss_test/(no_of_patches_x_test), (accuracy_test/no_of_patches_x_test), time.time() - start_time))\n",
        "  print('\\t\\t Mean F1 Score: {}'.format(mean_f1_score))\n",
        "  test_loss.append(total_loss_test/(no_of_patches_x_test))\n",
        "  test_accuracy.append(accuracy_test/(no_of_patches_x_test))\n",
        "\n",
        "  model_name = 'state_dict_epoch-'+str(epoch)+'_test_acc-' + str(\"{:.4f}\".format(accuracy_test/(no_of_patches_x_test)))+ '_mean_f1_score-'+ str(\"{:.4f}\".format(mean_f1_score))+'_'+str(experiment_id)+'.pt'\n",
        "  torch.save(model.state_dict(), os.path.join(model_folder,  model_name))\n",
        "  print('Saved model at', str(os.path.join(model_folder,  model_name)) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT41GtmPV47R"
      },
      "source": [
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.plot(train_loss, label=\"train loss\")\n",
        "plt.plot(test_loss, label=\"test loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(os.path.join(model_folder, ('loss_'+experiment_id+'_pytorch.png')))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.plot(train_accuracy, label=\"train acc\")\n",
        "plt.plot(test_accuracy, label=\"test acc\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(os.path.join(model_folder, ('accuracy_'+experiment_id+'_pytorch.png')))\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}