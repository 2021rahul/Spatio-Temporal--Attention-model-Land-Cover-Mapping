{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sentinel_spatio_temporal_attention_segmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzaPLyR7I8RU"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import gdal\n",
        "from osgeo import gdal, gdalconst, osr\n",
        "import time\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "# !nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMLYfexjuc1X"
      },
      "source": [
        "experiment_id = 'sentinel_2A_2018_T11SKA_spatio_temporal_attention_segmentation'\n",
        "\n",
        "quadrant_size = (5490,5490)\n",
        "grid_cell_size = (1372,1372)\n",
        "patch_size = (32,32)\n",
        "input_patch_size = 32\n",
        "label_patch_size = (16,16)\n",
        "timestamps = [0,2,3,5,8,11,14,17,19,22,24,26,28,30]                             # Timestamps to consider\n",
        "no_timestamps = time_steps = len(timestamps)\n",
        "no_features = channels = 10                                                     # Number of Features\n",
        "step_size = 16\n",
        "output_patch_width = 16\n",
        "no_of_grid_cells_x = 4\n",
        "no_of_grid_cells_y = 4\n",
        "batch_size = 16\n",
        "diff = 8\n",
        "total_no_of_grid_cells = no_of_grid_cells_x * no_of_grid_cells_y\n",
        "grids_train = [0,2,5,7,8,10,13,15]\n",
        "grids_test = [1,3,4,6,9,11,12,14]\n",
        "no_of_classes = 20\n",
        "labels_list = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
        "learning_rate = 0.0001\n",
        "max_accuracy_test = 0 \n",
        "no_of_epochs = 100\n",
        "quadrant = 4\n",
        "data_dir = 'data/sentinel/sentinel_2A_2018_T11SKA/numpy_arrays/quadrant_wise/quadrant' + str(quadrant)\n",
        "numpy_array_prefix = 'sentinel_2A_2018_T11SKA_series_quadrant4_patch'\n",
        "label_dir = 'data/sentinel/sentinel_2A_2018_T11SKA/labels/quadrant_wise/quadrant' + str(quadrant)\n",
        "label_array_prefix = 'sentinel_2A_2018_T11SKA_raw_label_quadrant4_patch'\n",
        "model_folder = 'models_sentinel_2A_2018_T11SKA_spatio_temporal_attention_segmentation/' + experiment_id\n",
        "if not os.path.exists(model_folder):\n",
        "  os.makedirs(model_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVQZ6htfAWbF"
      },
      "source": [
        "#label conversion from usda raw labels\n",
        "def convert_to_label_array(raw_label,label):\n",
        "  if(label.shape != raw_label.shape):\n",
        "    print(\"Shapes not equal\")\n",
        "  label[(raw_label==1) | (raw_label==225) | (raw_label==226) | (raw_label==237)] = 1    # Corn                 \n",
        "  label[(raw_label==2) | (raw_label==238)] = 2                                          # Cotton\n",
        "  label[(raw_label==4) | (raw_label==236)] = 3                                          # Sorghum                                                                           \n",
        "  label[(raw_label==22) | (raw_label==23) | (raw_label==24)] = 4                        # Wheat \n",
        "  label[(raw_label==36)] = 5                                                            # Alfa alfa\n",
        "  label[(raw_label==67)] = 6                                                            # Peaches\n",
        "  label[(raw_label==69)] = 7                                                            # Grapes\n",
        "  label[(raw_label==71)] = 8                                                            # Tree crops\n",
        "  label[(raw_label==72)] = 9                                                            # Citrus\n",
        "  label[(raw_label==75)] = 10                                                           # Almonds\n",
        "  label[(raw_label==76)] = 11                                                           # Walnut\n",
        "  label[(raw_label==204)] = 12                                                          # Pistachio\n",
        "  label[(raw_label==212)] = 13                                                          # Oranges\n",
        "  label[(raw_label==218)] = 14                                                          # Nectarines\n",
        "  label[(raw_label==5) | (raw_label==3) | (raw_label==27) | (raw_label==28) | (raw_label==44) | (raw_label==53) | (raw_label==21) | (raw_label==33) |(raw_label==42) | (raw_label==205)] = 15 # Misc Crops and Veg\n",
        "  label[(raw_label==37) | (raw_label==58) | (raw_label==59) | (raw_label==61) | (raw_label==152) | (raw_label==176) | (raw_label==190) | (raw_label==195)] = 16                               # Wetlands and Grass\n",
        "  label[(raw_label==61) | (raw_label==131)] = 17                                        # Barren/Idle land\n",
        "  label[(raw_label==111)] = 18                                                          # Water\n",
        "  label[(raw_label==121) | (raw_label==122) | (raw_label==123) | (raw_label==124)] = 19 # Urban\n",
        "  return label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8MD59Wds-_h"
      },
      "source": [
        "# Model Architecture\n",
        "class UNET_LSTM_BIDIRECTIONAL_ATTENTION(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNET_LSTM_BIDIRECTIONAL_ATTENTION,self).__init__()\n",
        "\n",
        "        self.conv1_1 = torch.nn.Conv2d(in_channels, 64, 3, padding=1)\n",
        "        self.conv1_2 = torch.nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.conv2_1 = torch.nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv2_2 = torch.nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.conv3_1 = torch.nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv3_2 = torch.nn.Conv2d(256, 256, 3, padding=1)\n",
        "\n",
        "        self.lstm = torch.nn.LSTM(256, 256, batch_first=True, bidirectional=True)\n",
        "        self.attention = torch.nn.Linear(512, 1)\n",
        "\n",
        "        self.unpool2 = torch.nn.ConvTranspose2d(512 , 128, kernel_size=2, stride=2)\n",
        "        self.upconv2_1 = torch.nn.Conv2d(256, 128, 3, padding=1)\n",
        "        self.upconv2_2 = torch.nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.unpool1 = torch.nn.ConvTranspose2d(128 , 64, kernel_size=2, stride=2)\n",
        "        self.upconv1_1 = torch.nn.Conv2d(128, 64, 3, padding=1)\n",
        "        self.upconv1_2 = torch.nn.Conv2d(64, 64, 3, padding=1)\n",
        "\n",
        "        self.out = torch.nn.Conv2d(64, out_channels, kernel_size=1, padding=0)\n",
        "\n",
        "        self.maxpool = torch.nn.MaxPool2d(2)\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "        self.dropout = torch.nn.Dropout(p=0.1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)        \n",
        "    \n",
        "    def crop_and_concat(self, x1, x2):\n",
        "        x1_shape = x1.shape\n",
        "        x2_shape = x2.shape\n",
        "        offset_2, offset_3 = (x1_shape[2]-x2_shape[2])//2, (x1_shape[3]-x2_shape[3])//2\n",
        "        x1_crop = x1[:, :, offset_2:offset_2+x2_shape[2], offset_3:offset_3+x2_shape[3]]\n",
        "        return torch.cat([x1_crop, x2], dim=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "      \n",
        "        x = x.view(-1, channels, input_patch_size, input_patch_size)\n",
        "\n",
        "        conv1 = self.relu(self.conv1_2(self.relu(self.conv1_1(x))))\n",
        "        maxpool1 = self.maxpool(conv1)\n",
        "        conv2 = self.relu(self.conv2_2(self.relu(self.conv2_1(maxpool1))))\n",
        "        maxpool2 = self.maxpool(conv2)\n",
        "        conv3 = self.relu(self.conv3_2(self.relu(self.conv3_1(maxpool2))))\n",
        "\n",
        "        shape_enc = conv3.shape \n",
        "        conv3 = conv3.view(-1, time_steps, conv3.shape[1], conv3.shape[2]*conv3.shape[3])\n",
        "        conv3 = conv3.permute(0,3,1,2) \n",
        "        conv3 = conv3.reshape(conv3.shape[0]*conv3.shape[1], time_steps, 256) \n",
        "        lstm, _ = self.lstm(conv3) \n",
        "        lstm = self.relu(lstm.reshape(-1, 512)) \n",
        "        attention_weights = torch.nn.functional.softmax(torch.squeeze(torch.nn.functional.avg_pool2d(self.attention(torch.tanh(lstm)).view(-1,shape_enc[2],shape_enc[3],time_steps).permute(0,3,1,2), 8)), dim=1)\n",
        "        context = torch.sum((attention_weights.view(-1, 1, 1, time_steps).repeat(1, 8, 8, 1).view(-1, 1)*lstm).view(-1, time_steps, 512), dim=1).view(-1,shape_enc[2],shape_enc[3], 512).permute(0,3,1,2) \n",
        "\n",
        "        attention_weights_fixed = attention_weights.detach()\n",
        "        unpool2 = self.unpool2(context)\n",
        "        agg_conv2 = torch.sum(attention_weights_fixed.view(-1, time_steps, 1, 1, 1) * conv2.view(-1, time_steps, conv2.shape[1], conv2.shape[2], conv2.shape[3]), dim=1)\n",
        "        upconv2 = self.relu(self.upconv2_2(self.relu(self.upconv2_1(self.crop_and_concat(agg_conv2, unpool2)))))\n",
        "        unpool1 = self.unpool1(upconv2)\n",
        "        agg_conv1 = torch.sum(attention_weights_fixed.view(-1, time_steps, 1, 1, 1) * conv1.view(-1, time_steps, conv1.shape[1], conv1.shape[2], conv1.shape[3]), dim=1)\n",
        "        upconv1 = self.relu(self.upconv1_2(self.relu(self.upconv1_1(self.crop_and_concat(agg_conv1, unpool1)))))\n",
        "        out = self.out(upconv1)\n",
        "\n",
        "        return out[:,:,diff:-diff, diff:-diff]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxT4_q_Hn5xb"
      },
      "source": [
        "# build model\n",
        "model = UNET_LSTM_BIDIRECTIONAL_ATTENTION(in_channels=no_features, out_channels=no_of_classes)\n",
        "model = model.to('cuda')\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "## train model\n",
        "train_loss = []\n",
        "train_accuracy = []\n",
        "test_loss = []\n",
        "test_accuracy = []\n",
        "no_of_patches_x = int((grid_cell_size[0] - (patch_size[0]))/step_size)\n",
        "no_of_patches_y = int((grid_cell_size[1] - (patch_size[1]))/step_size)\n",
        "no_of_patches_x_test = int((grid_cell_size[0] - (patch_size[0]))/step_size)\n",
        "no_of_patches_y_test = int((grid_cell_size[1] - (patch_size[1]))/step_size)\n",
        "w = int((patch_size[0]-label_patch_size[0])/2)\n",
        "no_of_batches_y = int(no_of_patches_y/batch_size)\n",
        "no_of_batches_y_test = int(no_of_patches_y_test/batch_size)\n",
        "image_batch = np.zeros((batch_size, ) + (no_timestamps, ) + patch_size + (no_features, ))\n",
        "label_batch = np.zeros((batch_size, ) + label_patch_size)\n",
        "print(no_of_patches_x,no_of_batches_y, image_batch.shape)\n",
        "\n",
        "for epoch in range(no_of_epochs):\n",
        "  print('\\n##  EPOCH ',epoch,'  ##')\n",
        "  \n",
        "  # Train  \n",
        "  print('\\tTraining')\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  accuracy_grids = 0\n",
        "  start_time = time.time()\n",
        "\n",
        "  for grid in grids_train:\n",
        "\n",
        "    start_grid_time = time.time()\n",
        "    accuracy_rows = 0\n",
        "    train_grid_cell = np.load(os.path.join(data_dir,numpy_array_prefix + str(grid) + '.npy'))\n",
        "    train_grid_cell_raw_label = np.load(os.path.join(label_dir,label_array_prefix + str(grid) + '.npy'))\n",
        "    train_grid_cell_label = np.zeros((grid_cell_size))\n",
        "    train_grid_cell_label = convert_to_label_array(train_grid_cell_raw_label,train_grid_cell_label)\n",
        "\n",
        "    for x in range(no_of_patches_x): \n",
        "\n",
        "      accuracy = 0\n",
        "\n",
        "      for y in range(no_of_batches_y):\n",
        "\n",
        "        for b in range(batch_size):\n",
        "          image_batch[b] = train_grid_cell[timestamps, x*step_size:(x*step_size) + patch_size[0], (((y*batch_size)+b)*step_size):(((y*batch_size)+b)*step_size) + patch_size[1], :] \n",
        "          label_batch[b] = train_grid_cell_label[x*step_size + w:(x*step_size) + w + label_patch_size[0], (((y*batch_size)+b)*step_size) + w:(((y*batch_size)+b)*step_size) + w + label_patch_size[1]]\n",
        "\n",
        "        image_batch_tr = np.transpose(image_batch,(0,1,4,2,3)) \n",
        "        image_batch_t = torch.Tensor(image_batch_tr)\n",
        "        label_batch_t = torch.Tensor(label_batch)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        patch_out = model(image_batch_t.to('cuda'))\n",
        "        label_batch_t = label_batch_t.type(torch.long).to('cuda')\n",
        "        loss = criterion(patch_out, label_batch_t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        patch_out_pred =  torch.argmax(torch.nn.functional.softmax(patch_out, dim=1), dim=1)\n",
        "        patch_out_pred = np.reshape(patch_out_pred.cpu().numpy(), (-1))\n",
        "        label_batch_t = np.reshape(label_batch_t.cpu().numpy(), (-1))\n",
        "        accuracy += accuracy_score(patch_out_pred,label_batch_t)\n",
        "\n",
        "      accuracy_rows += accuracy/no_of_batches_y\n",
        "\n",
        "    print('\\t\\tGrid no: ', grid, '\\tAccuracy grid: ',accuracy_rows/no_of_patches_x, '\\t Time taken for Grid: ', time.time() - start_grid_time)\n",
        "    accuracy_grids += accuracy_rows/no_of_patches_x\n",
        "\n",
        "  print('\\n\\tTrain:\\t Loss: {1}\\t Accuracy: {2}\\t Time: {3}'.format(epoch, total_loss/(no_of_patches_x), (accuracy_grids/len(grids_train)), time.time() - start_time))\n",
        "  train_loss.append(total_loss/(no_of_patches_x))\n",
        "  train_accuracy.append(accuracy_grids/len(grids_train))\n",
        "\n",
        "  # Test\n",
        "  model.eval()\n",
        "  print('\\n\\tTesting')\n",
        "  total_loss_test = 0\n",
        "  accuracy_test_grids = 0\n",
        "  start_time_test = time.time()\n",
        "  pred_list = []\n",
        "  true_list = []\n",
        "\n",
        "  for grid in grids_test:\n",
        "\n",
        "    start_grid_time_test = time.time()\n",
        "    accuracy_test_row = 0\n",
        "    test_grid_cell = np.load(os.path.join(data_dir,numpy_array_prefix + str(grid) + '.npy'))\n",
        "    test_grid_cell_raw_label = np.load(os.path.join(label_dir,label_array_prefix + str(grid) + '.npy'))\n",
        "    test_grid_cell_label = np.zeros((grid_cell_size))\n",
        "    test_grid_cell_label = convert_to_label_array(test_grid_cell_raw_label,test_grid_cell_label)\n",
        "\n",
        "    for x in range(no_of_patches_x_test):\n",
        "\n",
        "      accuracy_test = 0\n",
        "      # print(str(x)+str('-'),end = '')\n",
        "\n",
        "      for y in range(no_of_batches_y_test):\n",
        "\n",
        "        for b in range(batch_size):\n",
        "          image_batch[b] = test_grid_cell[timestamps, x*step_size:(x*step_size) + patch_size[0], (((y*batch_size)+b)*step_size):(((y*batch_size)+b)*step_size) + patch_size[1], :] \n",
        "          label_batch[b] = test_grid_cell_label[x*step_size + w:(x*step_size) + w + label_patch_size[0], (((y*batch_size)+b)*step_size) + w:(((y*batch_size)+b)*step_size) + w + label_patch_size[1]]\n",
        "      \n",
        "        image_batch_tr = np.transpose(image_batch,(0,1,4,2,3)) \n",
        "        image_batch_t = torch.Tensor(image_batch_tr)\n",
        "        label_batch_t = torch.Tensor(label_batch)\n",
        "\n",
        "        patch_out = model(image_batch_t.to('cuda'))\n",
        "        label_batch_t = label_batch_t.type(torch.long).to('cuda')\n",
        "        loss_test = criterion(patch_out, label_batch_t)\n",
        "        patch_out_pred =  torch.argmax(torch.nn.functional.softmax(patch_out, dim=1), dim=1)\n",
        "        total_loss_test += loss_test.item()\n",
        "        patch_out_pred = np.reshape(patch_out_pred.cpu().numpy(), (-1))\n",
        "        label_batch_t = np.reshape(label_batch_t.cpu().numpy(), (-1))\n",
        "        accuracy_test += accuracy_score(patch_out_pred,label_batch_t)\n",
        "        pred_list.append(patch_out_pred)\n",
        "        true_list.append(label_batch_t)\n",
        "\n",
        "      accuracy_test_row += accuracy_test/no_of_batches_y_test\n",
        "\n",
        "    print('\\t\\tGrid no: ', grid, '\\tAccuracy grid: ',accuracy_test_row/no_of_patches_x_test, '\\t Time taken for Grid: ', time.time() - start_grid_time_test)\n",
        "    accuracy_test_grids += accuracy_test_row/no_of_patches_x_test\n",
        "\n",
        "  pred_list_arr = np.array(pred_list).reshape(-1)\n",
        "  true_list_arr = np.array(true_list).reshape(-1)\n",
        "  mean_f1_score = np.mean(f1_score(true_list_arr,pred_list_arr,average = None,labels=labels_list))\n",
        "  print('\\tTest:\\t Loss: {}\\t Accuracy: {}\\t Time: {}'.format(total_loss_test/(no_of_patches_x_test), (accuracy_test_grids/len(grids_test)), time.time() - start_time_test))\n",
        "  print('\\t\\t\\t Mean F1 Score: {}'.format(mean_f1_score))\n",
        "  test_loss.append(total_loss_test/(no_of_patches_x_test))\n",
        "  test_accuracy.append(accuracy_test_grids/len(grids_test))\n",
        "\n",
        "  model_name = 'state_dict_epoch-'+str(epoch)+'_test_acc-' + str(\"{:.4f}\".format(accuracy_test_grids/len(grids_test))) + '_mean_f1_score-'+ str(\"{:.4f}\".format(mean_f1_score))+'_'+str(experiment_id)+'.pt'\n",
        "  torch.save(model.state_dict(), os.path.join(model_folder,  model_name))\n",
        "  print('Saved model at', str(os.path.join(model_folder,  model_name)) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT41GtmPV47R"
      },
      "source": [
        "# Plot graphs\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.plot(train_loss, label=\"train loss\")\n",
        "plt.plot(test_loss, label=\"test loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(os.path.join(model_folder, ('loss_'+experiment_id+'_pytorch.png')))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.plot(train_accuracy, label=\"train acc\")\n",
        "plt.plot(test_accuracy, label=\"test acc\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(os.path.join(model_folder, ('accuracy_'+experiment_id+'_pytorch.png')))\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}